from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
import pandas as pd
import numpy as np
import joblib
import json
import os
from datetime import datetime, timedelta
import random
import logging
import time
import sys

# Adicionar pastas ao path
sys.path.append('features')
sys.path.append('models')

from engineering import AdvancedFeatureEngine
from ensemble import FraudEnsembleModel

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Fraud Detection ML Service - ENSEMBLE",
    description="Sistema de Detecção de Fraudes com Ensemble (Random Forest + Isolation Forest)",
    version="3.0.0"
)

# Global variables
ensemble_model = FraudEnsembleModel()
processing_times = []
feature_engine = AdvancedFeatureEngine()

class Transaction(BaseModel):
    id: str
    user_id: str
    amount: float
    merchant: str
    location: Dict[str, float]
    timestamp: str
    card_type: str = "credit"
    transaction_type: str = "purchase"

class EnsembleFraudPrediction(BaseModel):
    transaction_id: str
    ensemble_score: float
    rf_score: float
    isolation_score: float
    risk_level: str
    model_agreement: str
    confidence: str
    reasons: List[str]
    processing_time_ms: int
    timestamp: str
    feature_count: int
    top_risk_factors: List[str]

def generate_synthetic_data_ensemble(n_samples: int = 15000):
    """Gera dataset maior e mais variado para ensemble"""
    logger.info(f"Gerando {n_samples} transações sintéticas para ENSEMBLE...")
    
    np.random.seed(42)
    random.seed(42)
    
    data = []
    
    # Merchants expandidos por categoria
    merchants = {
        'very_low_risk': ["McDonald's", "Subway", "Starbucks", "Carrefour", "Extra", "Pão de Açúcar"],
        'low_risk': ["Amazon", "Mercado Livre", "Magazine Luiza", "Americanas", "Netflix", "Spotify"],
        'medium_risk': ["Shell", "Petrobras", "ATM Banco", "Transfer Online", "Uber", "99"],
        'high_risk': ["Crypto Exchange", "Online Betting", "Cash Advance"],
        'very_high_risk': ["Underground Casino", "Dark Web", "Suspicious ATM"]
    }
    
    all_merchants = []
    for category in merchants.values():
        all_merchants.extend(category)
    
    card_types = ["credit", "debit", "prepaid"]
    transaction_types = ["purchase", "withdrawal", "transfer", "payment", "refund"]
    
    # Cidades brasileiras expandidas
    cities = {
        "São Paulo": {"lat": -23.5505, "lng": -46.6333, "risk": 0.03},
        "Rio de Janeiro": {"lat": -22.9068, "lng": -43.1729, "risk": 0.04},
        "Brasília": {"lat": -15.7801, "lng": -47.9292, "risk": 0.02},
        "Salvador": {"lat": -12.9714, "lng": -38.5014, "risk": 0.03},
        "Recife": {"lat": -8.0476, "lng": -34.8770, "risk": 0.03},
        "Frontier Town": {"lat": -10.0000, "lng": -50.0000, "risk": 0.15},  # Área de risco
        "Border City": {"lat": -25.0000, "lng": -55.0000, "risk": 0.20}     # Fronteira
    }
    
    # Perfis de usuários mais complexos
    user_profiles = {}
    for user_id in range(1, 1001):  # 1000 usuários
        risk_profile = random.choice(['low_risk', 'medium_risk', 'high_risk'])
        
        if risk_profile == 'low_risk':
            profile = {
                'home_city': random.choice(['São Paulo', 'Rio de Janeiro', 'Brasília']),
                'avg_amount': random.uniform(50, 200),
                'preferred_hours': random.sample(range(8, 20), 3),
                'favorite_merchants': random.sample(merchants['very_low_risk'] + merchants['low_risk'], 4),
                'fraud_probability': 0.02
            }
        elif risk_profile == 'medium_risk':
            profile = {
                'home_city': random.choice(list(cities.keys())),
                'avg_amount': random.uniform(100, 500),
                'preferred_hours': random.sample(range(6, 23), 4),
                'favorite_merchants': random.sample(merchants['low_risk'] + merchants['medium_risk'], 4),
                'fraud_probability': 0.05
            }
        else:  # high_risk
            profile = {
                'home_city': random.choice(['Frontier Town', 'Border City']),
                'avg_amount': random.uniform(200, 1000),
                'preferred_hours': random.sample(range(0, 24), 5),
                'favorite_merchants': random.sample(merchants['medium_risk'] + merchants['high_risk'], 3),
                'fraud_probability': 0.15
            }
        
        user_profiles[f"user_{user_id}"] = profile
    
    for i in range(n_samples):
        # Escolher usuário
        user_id = f"user_{random.randint(1, 1000)}"
        profile = user_profiles[user_id]
        
        # Determinar se é fraude baseado no perfil do usuário
        is_fraud = random.random() < profile['fraud_probability']
        
        # Se for fraude, aumentar ainda mais a probabilidade para padrões óbvios
        if is_fraud and random.random() < 0.7:  # 70% das fraudes são "óbvias"
            # FRAUDES ÓBVIAS
            amount = random.uniform(2000, 15000)
            hour = random.choice([2, 3, 4, 23, 0, 1])
            merchant = random.choice(merchants['high_risk'] + merchants['very_high_risk'])
            city = random.choice(['Frontier Town', 'Border City'])
            card_type = random.choice(['debit', 'prepaid'])  # Incomum para valores altos
            transactions_last_hour = random.randint(8, 25)
            
        elif is_fraud:
            # FRAUDES SUTIS
            amount = random.uniform(500, 2000)
            hour = random.choice(profile['preferred_hours'] + [22, 23, 0, 1])
            merchant = random.choice(merchants['medium_risk'] + merchants['high_risk'])
            city = profile['home_city'] if random.random() < 0.4 else random.choice(list(cities.keys()))
            card_type = random.choice(card_types)
            transactions_last_hour = random.randint(4, 10)
            
        else:
            # TRANSAÇÕES NORMAIS
            amount_base = profile['avg_amount']
            amount = np.random.normal(amount_base, amount_base * 0.3)
            amount = max(5, amount)
            
            hour = random.choice(profile['preferred_hours']) if random.random() < 0.8 else random.randint(6, 22)
            merchant = random.choice(profile['favorite_merchants']) if random.random() < 0.7 else random.choice(merchants['low_risk'])
            city = profile['home_city'] if random.random() < 0.9 else random.choice(['São Paulo', 'Rio de Janeiro'])
            card_type = "credit" if random.random() < 0.8 else "debit"
            transactions_last_hour = random.randint(0, 3)
        
        # Timestamp
        days_ago = random.randint(0, 90)
        timestamp = datetime.now() - timedelta(
            days=days_ago,
            hours=random.randint(0, 23),
            minutes=random.randint(0, 59)
        )
        timestamp = timestamp.replace(hour=hour)
        
        # Localização
        base_location = cities[city]
        location = {
            "lat": base_location["lat"] + random.uniform(-0.02, 0.02),
            "lng": base_location["lng"] + random.uniform(-0.02, 0.02)
        }
        
        transaction = {
            "id": f"txn_{i+1:06d}",
            "user_id": user_id,
            "amount": round(amount, 2),
            "merchant": merchant,
            "location": location,
            "timestamp": timestamp.isoformat(),
            "card_type": card_type,
            "transaction_type": random.choice(transaction_types),
            "is_fraud": int(is_fraud)
        }
        
        data.append(transaction)
    
    df = pd.DataFrame(data)
    
    # Adicionar algumas fraudes sintéticas específicas para testar edge cases
    fraud_count = df['is_fraud'].sum()
    normal_count = len(df) - fraud_count
    
    logger.info(f"Dataset ENSEMBLE gerado: {len(df)} transações, "
               f"{fraud_count} fraudes ({fraud_count/len(df)*100:.1f}%), "
               f"{normal_count} normais")
    
    return df

def prepare_ensemble_features(df):
    """Prepara features para o ensemble"""
    logger.info("Preparando features para ENSEMBLE...")
    
    all_features = []
    user_groups = df.groupby('user_id')
    
    for _, transaction in df.iterrows():
        # Histórico do usuário
        user_history = df[
            (df['user_id'] == transaction['user_id']) & 
            (df.index != transaction.name)
        ].to_dict('records')
        
        # Features avançadas
        features = feature_engine.engineer_advanced_features(
            transaction.to_dict(), 
            user_history if len(user_history) > 0 else None
        )
        
        all_features.append(features)
    
    # Converter para DataFrame
    features_df = pd.DataFrame(all_features)
    feature_names = feature_engine.get_feature_names()
    
    # Garantir ordem e presença das features
    for name in feature_names:
        if name not in features_df.columns:
            features_df[name] = 0
    
    return features_df[feature_names].values, feature_names

def train_ensemble_model():
    """Treina o modelo ensemble"""
    logger.info("Iniciando treinamento do ENSEMBLE...")
    
    # Gerar dataset maior
    df = generate_synthetic_data_ensemble(15000)
    
    # Preparar features
    X, feature_names = prepare_ensemble_features(df)
    y = df['is_fraud'].values
    
    # Treinar ensemble
    metrics = ensemble_model.train(X, y, feature_names)
    
    # Salvar modelo
    ensemble_model.save_model()
    
    logger.info("ENSEMBLE treinado e salvo!")
    return metrics

@app.on_event("startup")
async def startup_event():
    """Carrega ou treina ensemble na inicialização"""
    if not ensemble_model.load_model():
        logger.info("Ensemble não encontrado. Treinando...")
        train_ensemble_model()

@app.get("/")
async def root():
    return {
        "message": "Fraud Detection ML Service - ENSEMBLE",
        "status": "running",
        "model_loaded": ensemble_model.is_trained,
        "version": "3.0.0",
        "features": len(ensemble_model.feature_names),
        "models": ["Random Forest", "Isolation Forest"]
    }

@app.post("/predict", response_model=EnsembleFraudPrediction)
async def predict_fraud_ensemble(transaction: Transaction):
    """Predição com ensemble"""
    start_time = time.time()
    
    if not ensemble_model.is_trained:
        raise HTTPException(status_code=503, detail="Ensemble model not loaded")
    
    try:
        # Feature engineering
        features = feature_engine.engineer_advanced_features(transaction.dict())
        
        # Preparar array para o modelo
        feature_array = []
        for name in ensemble_model.feature_names:
            feature_array.append(features.get(name, 0))
        
        feature_array = np.array([feature_array])
        
        # Predição com ensemble
        prediction = ensemble_model.predict(feature_array)
        
        # Análise de razões específicas
        risk_factors = []
        top_risk_factors = []
        
        # Analisar features de alto risco
        if features.get('amount_above_max', 0):
            risk_factors.append("Valor acima do máximo histórico")
            top_risk_factors.append("amount_above_max")
        
        if features.get('very_far_from_home', 0):
            risk_factors.append("Localização muito distante do padrão")
            top_risk_factors.append("very_far_from_home")
        
        if features.get('is_unusual_time', 0):
            risk_factors.append("Horário fora do padrão do usuário")
            top_risk_factors.append("is_unusual_time")
        
        if features.get('merchant_risk_category', 0) >= 2:
            risk_factors.append("Merchant de alto risco")
            top_risk_factors.append("merchant_risk_category")
        
        if features.get('velocity_risk', 0):
            risk_factors.append("Múltiplas transações em pouco tempo")
            top_risk_factors.append("velocity_risk")
        
        if features.get('anomaly_indicators', 0) >= 3:
            risk_factors.append("Múltiplos indicadores anômalos detectados")
            top_risk_factors.append("anomaly_indicators")
        
        # Explicações baseadas no ensemble
        ensemble_explanations = ensemble_model.explain_prediction(prediction, features)
        
        # Razões base
        base_reasons = []
        if prediction["risk_level"] == "HIGH":
            base_reasons = [
                "Ensemble detectou alta probabilidade de fraude",
                f"Concordância entre modelos: {prediction['model_agreement']}",
                f"Confiança: {prediction['confidence']}"
            ]
        elif prediction["risk_level"] == "MEDIUM":
            base_reasons = [
                "Ensemble detectou risco moderado",
                "Alguns indicadores suspeitos presentes"
            ]
        else:
            base_reasons = [
                "Ensemble classificou como baixo risco",
                "Padrão normal detectado pelos modelos"
            ]
        
        # Combinar todas as razões
        all_reasons = base_reasons + risk_factors + ensemble_explanations
        
        # Calcular tempo de processamento
        processing_time = int((time.time() - start_time) * 1000)
        processing_times.append(processing_time)
        
        if len(processing_times) > 100:
            processing_times.pop(0)
        
        result = EnsembleFraudPrediction(
            transaction_id=transaction.id,
            ensemble_score=prediction["ensemble_score"],
            rf_score=prediction["rf_score"],
            isolation_score=prediction["isolation_score"],
            risk_level=prediction["risk_level"],
            model_agreement=prediction["model_agreement"],
            confidence=prediction["confidence"],
            reasons=all_reasons[:6],  # Máximo 6 razões
            processing_time_ms=processing_time,
            timestamp=datetime.now().isoformat(),
            feature_count=len(ensemble_model.feature_names),
            top_risk_factors=top_risk_factors[:3]
        )
        
        logger.info(f"ENSEMBLE PREDICTION: {transaction.id} -> {prediction['risk_level']} "
                   f"(ensemble: {prediction['ensemble_score']:.3f}, rf: {prediction['rf_score']:.3f}, "
                   f"iso: {prediction['isolation_score']:.3f}, {processing_time}ms)")
        
        return result
        
    except Exception as e:
        logger.error(f"Erro na predição ensemble: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Ensemble prediction error: {str(e)}")

@app.post("/retrain-ensemble")
async def retrain_ensemble():
    """Retreina o ensemble"""
    try:
        metrics = train_ensemble_model()
        return {
            "message": "Ensemble retrained successfully",
            "metrics": metrics,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Ensemble retraining error: {str(e)}")

@app.get("/metrics-ensemble")
async def get_ensemble_metrics():
    """Métricas detalhadas do ensemble"""
    avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0
    
    return {
        "ensemble_metrics": ensemble_model.metrics,
        "system_metrics": {
            "avg_processing_time_ms": round(avg_processing_time, 2),
            "predictions_count": len(processing_times),
            "ensemble_loaded": ensemble_model.is_trained,
            "feature_count": len(ensemble_model.feature_names),
            "models": ["Random Forest", "Isolation Forest"],
            "rf_weight": ensemble_model.rf_weight,
            "iso_weight": ensemble_model.iso_weight
        },
        "feature_analysis": ensemble_model.get_feature_importance_analysis(),
        "timestamp": datetime.now().isoformat()
    }

@app.get("/model-comparison")
async def compare_models():
    """Compara performance dos modelos individuais vs ensemble"""
    if not ensemble_model.is_trained:
        return {"error": "Ensemble not trained"}
    
    metrics = ensemble_model.metrics
    
    return {
        "random_forest": {
            "precision": metrics["rf_precision"],
            "recall": metrics["rf_recall"],
            "f1_score": metrics["rf_f1"],
            "roc_auc": metrics["rf_roc_auc"],
            "oob_score": metrics["rf_oob_score"]
        },
        "isolation_forest": {
            "precision": metrics["iso_precision"],
            "recall": metrics["iso_recall"],
            "f1_score": metrics["iso_f1"]
        },
        "ensemble": {
            "precision": metrics["ensemble_precision"],
            "recall": metrics["ensemble_recall"],
            "f1_score": metrics["ensemble_f1"],
            "roc_auc": metrics["ensemble_roc_auc"]
        },
        "best_performer": {
            "f1_score": max(
                ("Random Forest", metrics["rf_f1"]),
                ("Isolation Forest", metrics["iso_f1"]),
                ("Ensemble", metrics["ensemble_f1"])
            ),
            "roc_auc": max(
                ("Random Forest", metrics["rf_roc_auc"]),
                ("Ensemble", metrics["ensemble_roc_auc"])
            )
        }
    }

@app.get("/generate-ensemble-test-data")
async def generate_ensemble_test_data():
    """Gera transações de teste específicas para ensemble"""
    test_transactions = []
    
    # Transação muito normal
    test_transactions.append({
        "id": "ensemble_normal_001",
        "user_id": "user_safe_123",
        "amount": 45.90,
        "merchant": "McDonald's",
        "location": {"lat": -23.5505, "lng": -46.6333},
        "timestamp": datetime.now().replace(hour=12).isoformat(),
        "card_type": "credit",
        "transaction_type": "purchase"
    })
    
    # Transação levemente suspeita (só RF vai detectar)
    test_transactions.append({
        "id": "ensemble_mild_002",
        "user_id": "user_safe_123",
        "amount": 850.00,  # Valor um pouco alto
        "merchant": "Amazon",
        "location": {"lat": -23.5505, "lng": -46.6333},
        "timestamp": datetime.now().replace(hour=22).isoformat(),  # Horário noturno
        "card_type": "credit",
        "transaction_type": "purchase"
    })
    
    # Transação anômala (só Isolation vai detectar)
    test_transactions.append({
        "id": "ensemble_anomaly_003",
        "user_id": "user_safe_123",
        "amount": 100.00,  # Valor normal
        "merchant": "Starbucks",  # Merchant normal
        "location": {"lat": -12.9714, "lng": -38.5014},  # Salvador (longe)
        "timestamp": datetime.now().replace(hour=14).isoformat(),  # Horário normal
        "card_type": "prepaid",  # Tipo incomum
        "transaction_type": "withdrawal"  # Tipo incomum
    })
    
    # Transação suspeita (ambos vão detectar)
    test_transactions.append({
        "id": "ensemble_suspicious_004",
        "user_id": "user_safe_123",
        "amount": 3500.00,  # Valor alto
        "merchant": "Crypto Exchange",  # Merchant de risco
        "location": {"lat": -8.0476, "lng": -34.8770},  # Recife (longe)
        "timestamp": datetime.now().replace(hour=3).isoformat(),  # Madrugada
        "card_type": "debit",  # Incomum para valor alto
        "transaction_type": "transfer"
    })
    
    # Transação MUITO fraudulenta (ensemble vai ter certeza)
    test_transactions.append({
        "id": "ensemble_fraud_005",
        "user_id": "user_safe_123",
        "amount": 9999.99,  # Valor máximo
        "merchant": "Dark Web",  # Merchant muito suspeito
        "location": {"lat": -10.0000, "lng": -50.0000},  # Lugar perigoso
        "timestamp": datetime.now().replace(hour=2).isoformat(),  # Madrugada
        "card_type": "prepaid",
        "transaction_type": "withdrawal"
    })
    
    return {
        "test_transactions": test_transactions,
        "message": "Transações específicas para testar o ensemble",
        "expected_results": {
            "ensemble_normal_001": "LOW (ambos modelos vão classificar como normal)",
            "ensemble_mild_002": "MEDIUM (Random Forest detecta, Isolation pode não detectar)",
            "ensemble_anomaly_003": "MEDIUM (Isolation detecta anomalia, RF pode não detectar)",
            "ensemble_suspicious_004": "HIGH (ambos detectam, alta concordância)",
            "ensemble_fraud_005": "HIGH (ensemble com máxima confiança)"
        },
        "instructions": "Teste cada transação e compare os scores RF vs Isolation"
    }

@app.get("/feature-importance-detailed")
async def get_detailed_feature_importance():
    """Análise detalhada das features mais importantes"""
    if not ensemble_model.is_trained:
        return {"error": "Ensemble not trained"}
    
    analysis = ensemble_model.get_feature_importance_analysis()
    
    return {
        "feature_analysis": analysis,
        "feature_categories": {
            "value_features": [f for f in ensemble_model.feature_names if 'amount' in f],
            "temporal_features": [f for f in ensemble_model.feature_names if any(x in f for x in ['hour', 'time', 'day'])],
            "location_features": [f for f in ensemble_model.feature_names if any(x in f for x in ['location', 'distance', 'home'])],
            "behavioral_features": [f for f in ensemble_model.feature_names if any(x in f for x in ['merchant', 'card', 'user'])],
            "composite_features": [f for f in ensemble_model.feature_names if any(x in f for x in ['risk', 'anomaly', 'composite'])]
        },
        "recommendations": {
            "focus_on": "Features com importância > 5% são críticas para o modelo",
            "monitor": "Features com baixa importância podem ser removidas para otimização",
            "top_10_impact": f"{analysis.get('top_10_contribute_pct', 0):.1f}% da decisão vem das top 10 features"
        }
    }

@app.get("/health-ensemble")
async def health_check_ensemble():
    """Health check específico do ensemble"""
    rf_loaded = ensemble_model.random_forest is not None
    iso_loaded = ensemble_model.isolation_forest is not None
    
    return {
        "status": "healthy" if ensemble_model.is_trained else "degraded",
        "ensemble_trained": ensemble_model.is_trained,
        "random_forest_loaded": rf_loaded,
        "isolation_forest_loaded": iso_loaded,
        "feature_engine_ready": feature_engine is not None,
        "features_count": len(ensemble_model.feature_names),
        "last_prediction": len(processing_times) > 0,
        "models_agreement": "Available" if ensemble_model.is_trained else "N/A",
        "timestamp": datetime.now().isoformat()
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)